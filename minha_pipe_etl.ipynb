{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm4n/iVGWtJhBmOyGeoapD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AllanPin/minha_pipeline_etl/blob/main/minha_pipe_etl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contexto da minha pipeline:**\n",
        "\n",
        "- eu já atingi meu limite de tokens da OpenAI, então não posso me utilizar da API deles para poder enviar aquelas mensagens super legais e personalizadas;\n",
        "\n",
        "- nem por isso eu vou deixar de trazer criatividade. Resolvi fazer uma busca por vários modelos de ETL e montei um pipeline do meu jeito;\n",
        "\n",
        "- apesar de sair do contexto bancário, que era a proposta inicial do grande Venilton, porém a essência fica: o aprendizado;\n",
        "\n",
        "- ainda vamos ter uma api rest, com enfoque direto e indireto em ciência de dados."
      ],
      "metadata": {
        "id": "2BpyxHiDAmi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To do:\n",
        "\n",
        ">- criar minha api rest, para que possamos consumir seu endpoint como fonte de dados na nossa camada de extração;\n",
        "\n",
        ">- fazer a transformação/enriquecimento dos dados;\n",
        "\n",
        ">- enviar os dados de volta para a nossa api"
      ],
      "metadata": {
        "id": "gRIwNImcC7ev"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQJ6O6sW_x60"
      },
      "outputs": [],
      "source": [
        "# criar a api e salvar no swagger, será que dá certo?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extraindo** os dados de treinamento"
      ],
      "metadata": {
        "id": "BUpAr-aGEUSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras é uma API feita com Python e roda com base no TensorFlow, o modelo de deep learning feito pelo time do Google Brain. Ao lado do PyTorch, é um dos modelos mais utilizados nesse ramo.\n",
        "\n",
        "Keras conta com alguns datasets que podemos carregar como nossa fonte de dados primária. Dentre esses datasets, o que escolhemos foi o Fashion MNIST, uma base com 10 mil imagens para treinarmos o modelo e mais 60 mil para testarmos o modelo.\n",
        "\n",
        "Esse Fashion MNIST, como o nome sugere, possui imagens cujos rótulos são categorizados da seguinte maneira:\n",
        "\n",
        ">| Rótulo | Descrição | Tradução\n",
        " | -------| ----------| ----------|\n",
        " | 0 | T-shirt/top| Camiseta |\n",
        " | 1 | Trouser | Calças |\n",
        " | 2 | Pullover | Pulôver |\n",
        " | 3 | Dress | Vestido |\n",
        " | 4 | Coat | Casaco |\n",
        " | 5 | Sandal | Sandália |\n",
        " | 6 | Shirt | Camisa |\n",
        " | 7 | Sneaker | Tênis |\n",
        " | 8 | Bag | Bolsa |\n",
        " | 9 | Ankle boot | Bota de Salto |\n",
        "\n",
        " Ps.: markdown é genial!"
      ],
      "metadata": {
        "id": "Yp_vFyAJEkWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "(xtrain, ytrain), (xtest, ytest) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "xtrain = xtrain.astype('float32') / 255\n",
        "xtest = xtest.astype('float32') / 255\n",
        "\n",
        "xtrain = np.reshape(xtrain, (xtrain.shape[0], 28, 28, 1))\n",
        "xtest = np.reshape(xtest, (xtest.shape[0], 28, 28, 1))\n",
        "\n",
        "print(xtrain.shape)\n",
        "print(ytrain.shape)\n",
        "print(xtest.shape)\n",
        "print(xtest.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvSQ76R0D5Cl",
        "outputId": "5de84d46-70c6-4263-f6a5-8183db8c2e62"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(60000,)\n",
            "(10000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Carregando** e **Transformando** os dados em um banco de dados sql e em seguida um DataFrame do pandas"
      ],
      "metadata": {
        "id": "535QF0ghEZ9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nessa etapa, iremos armazenar os dados em um banco de dados SQL. Vamos utilizar o pandas também, para visualização das tabelas."
      ],
      "metadata": {
        "id": "ZsMNOO3HIDkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "1qOsCuO_JW5b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# faz a conexão com o sqlite3, através de database chamado fashion_mnist\n",
        "conn = sqlite3.connect('fashion_mnist.db')\n",
        "\n",
        "# criamos a tabela para receber as imagens\n",
        "# blob é literalmente uma bolha de dados. Isso faz sentido para as imagens\n",
        "# pois uma imagem nada mais é do que uma matriz de pixels. E, para contextualizar\n",
        "# melhor, as imagens de fashion_mnist são de gradientes de cinza, i.e., cada\n",
        "# pixel terá um valor individual entre 0 e 255, sendo 0 -> preto e 255 -> branco.\n",
        "# É importante destacar também que as imagens de nossa base são de resolução\n",
        "# 28 x 28, totalizando 784 pixels por imagem\n",
        "conn.execute('''CREATE TABLE IF NOT EXISTS images\n",
        "             (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "             image BLOB NOT NULL,\n",
        "             label INTEGER NOT NULL);''')\n",
        "\n",
        "# Esse loop é o responsável por \"popular\" a nossa tabela com as imagens e\n",
        "# seus respectivos rótulos. Os pontos de interrogação representam os valores que\n",
        "# serão inseridos nas colunas \"image\" e \"label\" da tabela \"images\"\n",
        "for i in range(xtrain.shape[0]):\n",
        "    conn.execute('INSERT INTO images (image, label) VALUES (?, ?)',\n",
        "                [sqlite3.Binary(xtrain[i]), ytrain[i]])\n",
        "\n",
        "# commitamos as mudanças feitas. Tipo no git (se eu entendesse git...)\n",
        "conn.commit()\n",
        "\n",
        "#O mesmo que o loop anterior, porém com as imagens de teste\n",
        "for i in range(xtest.shape[0]):\n",
        "    conn.execute('INSERT INTO images (image, label) VALUES (?, ?)',\n",
        "                [sqlite3.Binary(xtest[i]), ytest[i]])\n",
        "\n",
        "# commitamos novamente as inserções feitas\n",
        "conn.commit()\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "FCc3QxBlD55D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exemplo de como acessar os dados armazenados:"
      ],
      "metadata": {
        "id": "sK9bR6aQL8AR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conn = sqlite3.connect('fashion_mnist.db')\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('SELECT * FROM images')\n",
        "rows = cursor.fetchall()\n",
        "\n",
        "# passamos todas as imagens para um DataFrame do pandas\n",
        "# desse modo a gente consegue trabalhar com os dados mas sem o \"engessamento\"\n",
        "# da linguagem sql (perdão pela ofensa, dba's do mundo todo)\n",
        "data = pd.read_sql_query('SELECT * FROM images', conn)"
      ],
      "metadata": {
        "id": "31-biE1ZLyo3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusão**"
      ],
      "metadata": {
        "id": "OMjKcoYhNF9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esse pipeline ETL é um exemplo muito rico e que pode ser o fundamento para a elaboração e treinamento de um modelo de Deep Learning que possa \"visualizar\" imagens e nos dizer se há alguma peça de vestuário. Sem falar que poderíamos facilmente alterarmos a fonte de dados na seção de extração e mesmo assim conseguiríamos manter o perfil do restante da pipeline quase que intacto (ainda precisaríamos modificar alguns detalhes, como por exemplo o nome das variáveis para que o código mantenha um certo contexto)."
      ],
      "metadata": {
        "id": "JObNz6cENI64"
      }
    }
  ]
}